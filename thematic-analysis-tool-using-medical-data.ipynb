{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3041917,"sourceType":"datasetVersion","datasetId":1860871}],"dockerImageVersionId":31042,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Install needed libraries\n\n!pip install -q transformers torch sentencepiece ipywidgets\n!pip uninstall -y gensim\n!pip install --upgrade --force-reinstall gensim\n!pip install numpy==1.26.4\n!pip install gensim\n!pip install -q pymupdf\n!pip install nltk\n!pip uninstall -y spacy cython\n!pip install --no-cache-dir spacy cython\n!pip install kneed","metadata":{"id":"zkhyHu7n4-v7","outputId":"a6dcffff-72bf-466c-8317-353073b82288","trusted":true,"execution":{"iopub.status.busy":"2025-06-07T08:41:16.943372Z","iopub.execute_input":"2025-06-07T08:41:16.943950Z","iopub.status.idle":"2025-06-07T08:43:28.449192Z","shell.execute_reply.started":"2025-06-07T08:41:16.943928Z","shell.execute_reply":"2025-06-07T08:43:28.448427Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import libraires\n\nfrom huggingface_hub import login\nlogin(token=\"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\")\nimport os, io, logging, requests\nimport fitz  # PyMuPDF\nimport nltk\nimport kagglehub\nfrom bs4 import BeautifulSoup\nfrom IPython.display import display, clear_output\nimport ipywidgets as widgets\nfrom nltk.tokenize import sent_tokenize\nimport re\nimport spacy\nnltk.download('punkt')\nnltk.download('punkt_tab')\nnltk.download(\"vader_lexicon\")\nfrom transformers import BertTokenizer, BertModel\nimport torch\nimport numpy as np\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nimport matplotlib.pyplot as plt\nfrom kneed import KneeLocator\nfrom collections import Counter\nimport spacy\nfrom collections import defaultdict, Counter\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom nltk.sentiment import SentimentIntensityAnalyzer\nimport pandas as pd\nfrom wordcloud import WordCloud\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.decomposition import PCA\n# from sentence_transformers import SentenceTransformer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-07T08:44:14.384457Z","iopub.execute_input":"2025-06-07T08:44:14.385126Z","iopub.status.idle":"2025-06-07T08:44:40.994384Z","shell.execute_reply.started":"2025-06-07T08:44:14.385098Z","shell.execute_reply":"2025-06-07T08:44:40.993829Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Download dataset\npath = kagglehub.dataset_download(\"anshulmehtakaggl/200000-abstracts-for-seq-sentence-classification\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"id":"j_ZSNsgASj0I","outputId":"e81a8b4e-07ef-4b31-dc3c-50c4013ccca3","trusted":true,"execution":{"iopub.status.busy":"2025-06-07T08:45:34.090221Z","iopub.execute_input":"2025-06-07T08:45:34.091185Z","iopub.status.idle":"2025-06-07T08:45:34.449303Z","shell.execute_reply.started":"2025-06-07T08:45:34.091160Z","shell.execute_reply":"2025-06-07T08:45:34.448688Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"file_path = \"/kaggle/input/200000-abstracts-for-seq-sentence-classification/20k_abstracts/train.txt\"  # Change the path to match your file location\n\nwith open(file_path, \"r\", encoding=\"utf-8\") as f:\n    text_data = f.read()\n\nprint(text_data[:500])  # Preview the first 500 characters","metadata":{"id":"_hU6_ZOvSoCz","trusted":true,"execution":{"iopub.status.busy":"2025-06-07T08:45:42.970289Z","iopub.execute_input":"2025-06-07T08:45:42.970565Z","iopub.status.idle":"2025-06-07T08:45:43.246337Z","shell.execute_reply.started":"2025-06-07T08:45:42.970545Z","shell.execute_reply":"2025-06-07T08:45:43.245683Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# â”€â”€â”€ Setup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# Base directories & logging\nbase_dir = '/content/NLP_Thematic_Tool'\nraw_dir  = os.path.join(base_dir, 'raw_data')\nos.makedirs(raw_dir, exist_ok=True)\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s %(levelname)s %(message)s\",\n    handlers=[logging.FileHandler(os.path.join(base_dir, 'system.log')),\n              logging.StreamHandler()]\n)\n\n# Download NLTK data\nnltk.download('punkt')\nnltk.download('punkt')\nnltk.download('vader_lexicon')\n\n# â”€â”€â”€ Widgets â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\npdf_upload = widgets.FileUpload(\n    accept='.pdf', multiple=True, description=\"Upload PDFs\"\n)\nurl_input = widgets.Textarea(\n    placeholder='One URL per line', description='Web URLs:', layout=widgets.Layout(width='100%', height='100px')\n)\nprocess_btn = widgets.Button(description=\"Process Files\", button_style='success')\noutput_box  = widgets.Output()\n\ndisplay(widgets.VBox([\n    widgets.HTML(\"<h3>Step 1: Upload PDFs and/or enter URLs</h3>\"),\n    pdf_upload,\n    url_input,\n    process_btn,\n    output_box\n]))\n\n# â”€â”€â”€ Extraction Functions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ndef extract_text_from_pdfs(uploaded):\n    texts = []\n    \n    for file_tuple in uploaded:\n        fname, info = file_tuple  # Unpack tuple correctly\n        stream = io.BytesIO(info['content'])\n        doc = fitz.open(stream=stream, filetype='pdf')\n        text = \" \".join(page.get_text() for page in doc)\n        texts.append(text)\n        logging.info(f\"Extracted text from PDF: {fname}\")\n\n    return texts\n\n\ndef extract_text_from_urls(lines):\n    texts = []\n    for idx, url in enumerate(lines, 1):\n        try:\n            resp = requests.get(url, timeout=10)\n            soup = BeautifulSoup(resp.content, 'html.parser')\n            txt  = soup.get_text(separator=' ', strip=True)\n            texts.append(txt)\n            logging.info(f\"Scraped URL #{idx}: {url}\")\n        except Exception as e:\n            logging.error(f\"Error scraping {url}: {e}\")\n    return texts\n\n# â”€â”€â”€ Button Callback â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ndef on_process_clicked(btn):\n    with output_box:\n        clear_output()\n        # 1. Gather PDF texts\n        pdf_texts = extract_text_from_pdfs(pdf_upload.value)\n        print(f\"âœ” PDFs processed: {len(pdf_texts)}\")\n\n        # 2. Gather URL texts (one per line)\n        urls = [u.strip() for u in url_input.value.splitlines() if u.strip()]\n        url_texts = extract_text_from_urls(urls)\n        print(f\"âœ” URLs scraped:   {len(url_texts)}\")\n\n        # 3. Combine\n        all_texts = pdf_texts + url_texts\n        print(f\"Total sources:    {len(all_texts)}\\n\")\n\n        # Sanity check first 200 chars\n        for i, doc in enumerate(all_texts, 1):\n            print(f\"--- Source #{i} preview ---\")\n            print(doc[:200].replace('\\n',' ') + '...\\n')\n\n        # Continue with your downstream processing here...\n        # e.g. preprocess â†’ tokenize â†’ embed â†’ cluster â†’ visualize\n\nprocess_btn.on_click(on_process_clicked)","metadata":{"id":"evyEN8xb95xW","outputId":"a87f8b62-de2a-4574-b8ad-c00a9622bc5c","trusted":true,"execution":{"iopub.status.busy":"2025-06-07T08:45:50.711241Z","iopub.execute_input":"2025-06-07T08:45:50.711517Z","iopub.status.idle":"2025-06-07T08:45:50.740998Z","shell.execute_reply.started":"2025-06-07T08:45:50.711498Z","shell.execute_reply":"2025-06-07T08:45:50.740079Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Extract all text\npdf_texts = extract_text_from_pdfs(pdf_upload.value)\nurl_texts = extract_text_from_urls(url_input.value.strip().split('\\n') if url_input.value else [])\n\n# Define the file path for the TXT file\n\n# Read the TXT file into a list\n##with open(file_path, \"r\", encoding=\"utf-8\") as f:\n##    txt_texts = f.readlines()  # Reads each line as an item in a list\n\n# Strip unnecessary spaces and filter empty lines\ntxt_texts = [line.strip() for line in text_data.splitlines() if line.strip()]\n\n# Merge with other sources\nall_texts = pdf_texts + url_texts + txt_texts\n\n# Merge with excel texts\n#all_texts = pdf_texts + url_texts + excel_texts","metadata":{"id":"VwwrULNBBuTn","trusted":true,"execution":{"iopub.status.busy":"2025-06-07T08:48:57.496982Z","iopub.execute_input":"2025-06-07T08:48:57.497549Z","iopub.status.idle":"2025-06-07T08:49:01.173918Z","shell.execute_reply.started":"2025-06-07T08:48:57.497528Z","shell.execute_reply":"2025-06-07T08:49:01.173098Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess(text):\n    # Replace paragraph breaks with a space so sentences remain intact\n    text = text.replace(\"\\n\", \" \")\n\n    # Preserve punctuation while removing unwanted symbols\n    text = re.sub(r'[^a-zA-Z0-9.,!?;:\\s]', '', text)\n\n    return text.lower()\n\n\nsentences = []\nfor doc in all_texts:\n    preprocessed = preprocess(doc)\n    sentences.extend(sent_tokenize(preprocessed))  # Proper sentence splitting\n\n# Function to filter irrelevant sentences using spaCy\n\n# Load spaCy model for Named Entity Recognition\nnlp = spacy.load(\"en_core_web_sm\")\n\n\ndef is_relevant(sentence):\n    doc = nlp(sentence)\n\n    # Count named entities (proper nouns, organizations, etc.)\n    entity_count = sum(1 for ent in doc.ents if ent.label_ in [\"ORG\", \"PERSON\", \"GPE\", \"PRODUCT\"])\n\n    # Apply POS tagging to remove sentences dominated by function words\n    pos_tags = [token.pos_ for token in doc]\n    relevant_pos = sum(1 for tag in pos_tags if tag in [\"NOUN\", \"VERB\", \"ADJ\"])  # Keep nouns, verbs, adjectives\n\n    # Define relevance criteria\n    return entity_count < 2 and relevant_pos >= 3  # Adjust thresholds as needed\n\n# Apply spaCy filtering to remove irrelevant sentences\nfiltered_sentences = [s for s in sentences if is_relevant(s)]\nprint(f\"Total sentences extracted: {len(filtered_sentences)}\")","metadata":{"id":"2jOyx7O5zz-m","outputId":"9ba7eb54-8f55-4a81-dfc7-cd697da4c3c5","trusted":true,"execution":{"iopub.status.busy":"2025-06-07T08:49:39.207441Z","iopub.execute_input":"2025-06-07T08:49:39.207954Z","iopub.status.idle":"2025-06-07T09:14:01.283147Z","shell.execute_reply.started":"2025-06-07T08:49:39.207928Z","shell.execute_reply":"2025-06-07T09:14:01.282350Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n\nstopword_widget = widgets.Textarea(\n    value='',\n    placeholder='Enter extra stopwords separated by commas',\n    description='Stopwords:',\n    layout=widgets.Layout(width='100%', height='60px')\n)\ndisplay(stopword_widget)\n\ndef get_custom_stopwords():\n    additional = stopword_widget.value.strip()\n    custom_words = set(map(str.strip, additional.split(','))) if additional else set()\n    return set(ENGLISH_STOP_WORDS).union(custom_words)\n","metadata":{"id":"Ekdn_Fezz47e","outputId":"fd272f7a-ea3f-433b-bc6e-05ff4c362384","trusted":true,"execution":{"iopub.status.busy":"2025-06-07T09:28:45.834087Z","iopub.execute_input":"2025-06-07T09:28:45.834494Z","iopub.status.idle":"2025-06-07T09:28:45.842647Z","shell.execute_reply.started":"2025-06-07T09:28:45.834475Z","shell.execute_reply":"2025-06-07T09:28:45.841822Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"themes_widget = widgets.Textarea(\n    value='',  # Make it empty by default\n    placeholder='theme_name: keyword1, keyword2, ...',\n    description='Themes:',\n    layout=widgets.Layout(width='100%', height='150px')\n)\ndisplay(themes_widget)\n\ndef parse_themes():\n    raw = themes_widget.value.strip().split('\\n')\n    theme_dict = {}\n    for line in raw:\n        if ':' in line:\n            theme, keywords = line.split(':', 1)\n            theme_dict[theme.strip()] = [k.strip().lower() for k in keywords.split(',')]\n    return theme_dict\n","metadata":{"id":"FxUPF-wPz8dY","outputId":"c30489ff-d9c0-41ce-e028-500dcf9fd6e1","trusted":true,"execution":{"iopub.status.busy":"2025-06-07T09:29:41.958526Z","iopub.execute_input":"2025-06-07T09:29:41.958825Z","iopub.status.idle":"2025-06-07T09:29:41.967105Z","shell.execute_reply.started":"2025-06-07T09:29:41.958805Z","shell.execute_reply":"2025-06-07T09:29:41.966374Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Ensure GPU is used if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load BERT model & tokenizer\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\nmodel = BertModel.from_pretrained(\"bert-base-uncased\").to(device)  # Move model to GPU\n\n# Function to compute sentence embeddings\ndef compute_embeddings(sentences, batch_size=35):\n    embeddings = []\n    for i in range(0, len(sentences), batch_size):\n        batch = sentences[i:i + batch_size]\n        inputs = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\").to(device)  # Move input to GPU\n        with torch.no_grad():\n            outputs = model(**inputs)\n        embeddings.extend(outputs.last_hidden_state[:, 0, :].cpu().numpy())  # Move tensor back to CPU for NumPy\n    return np.array(embeddings)\n\n# Compute embeddings in batches\nsentence_embeddings = compute_embeddings(filtered_sentences, batch_size=35)","metadata":{"id":"L4mdOcUP8B1C","trusted":true,"execution":{"iopub.status.busy":"2025-06-07T09:29:52.674218Z","iopub.execute_input":"2025-06-07T09:29:52.674490Z","iopub.status.idle":"2025-06-07T09:38:38.874891Z","shell.execute_reply.started":"2025-06-07T09:29:52.674470Z","shell.execute_reply":"2025-06-07T09:38:38.874283Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def find_optimal_k(X, max_k=10):\n    inertia_values = []\n    cluster_range = range(2, max_k + 1)\n\n    for k in cluster_range:\n        km = KMeans(n_clusters=k, random_state=42)\n        km.fit(X)\n        inertia_values.append(km.inertia_)\n\n    # Use KneeLocator to find the best K\n    kn = KneeLocator(cluster_range, inertia_values, curve=\"convex\", direction=\"decreasing\")\n    optimal_k = kn.knee\n\n    plt.figure(figsize=(8, 5))\n    plt.plot(cluster_range, inertia_values, marker='o', linestyle='--')\n    plt.axvline(optimal_k, color='r', linestyle=\"--\", label=f\"Optimal K = {optimal_k}\")\n    plt.xlabel(\"Number of Clusters (K)\")\n    plt.ylabel(\"Inertia\")\n    plt.title(\"Elbow Method for Optimal K\")\n    plt.legend()\n    plt.show()\n\n    return optimal_k\n\nX = np.array(sentence_embeddings)\n\noptimal_k = find_optimal_k(X)\nprint(f\"Optimal K selected: {optimal_k}\")\n\n# Apply KMeans with optimal K\nkmeans = KMeans(n_clusters=optimal_k, random_state=42)\nlabels = kmeans.fit_predict(X)\n\n\n# Evaluate cluster quality\nsil_score = silhouette_score(X, labels)\nprint(f\"ğŸ”¹ Silhouette Score: {sil_score:.2f}\")\n\n# Assign sentences to clusters\nclustered_sentences = {i: [] for i in range(kmeans.n_clusters)}\nfor label, sentence in zip(labels, filtered_sentences):\n    clustered_sentences[label].append(sentence)\n\n# Display sentence clusters\nfor cluster, sentences in clustered_sentences.items():\n    print(f\"\\nğŸ”¹ Cluster {cluster}: {len(sentences)} sentences\")\n    print(\"\\n\".join(sentences[:5]))  # Show first 5 sentences per cluster","metadata":{"id":"VykTd1LJWcXl","outputId":"01ca70a2-2b69-4550-cc1e-a69b318ba581","trusted":true,"execution":{"iopub.status.busy":"2025-06-07T09:41:38.852843Z","iopub.execute_input":"2025-06-07T09:41:38.853131Z","iopub.status.idle":"2025-06-07T10:11:42.037265Z","shell.execute_reply.started":"2025-06-07T09:41:38.853111Z","shell.execute_reply":"2025-06-07T10:11:42.036467Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"theme_sentences = defaultdict(list)\n\nfor cluster, sentences in clustered_sentences.items():\n    theme_sentences[f\"Theme_{cluster}\"] = sentences\n\n# Display clusters as themes\nfor theme, sentences in theme_sentences.items():\n    print(f\"\\nğŸ”¹ {theme}: {len(sentences)} sentences\")\n\n\n\nstopwords = get_custom_stopwords()\nthemes = parse_themes()\ntheme_sentences = defaultdict(list)\ntheme_sentiment = {}\n\nfor sentence, emb in zip(filtered_sentences, sentence_embeddings):\n    sentence_lower = sentence.lower()\n    words = set(sentence_lower.split()) - stopwords\n    for theme, keywords in themes.items():\n        if any(kw in words for kw in keywords):\n            theme_sentences[theme].append(sentence)","metadata":{"id":"ybFaRHch2HQU","outputId":"f4585ee3-ec7e-4b78-e9a8-d923c3d90a89","trusted":true,"execution":{"iopub.status.busy":"2025-06-07T10:23:29.512474Z","iopub.execute_input":"2025-06-07T10:23:29.512828Z","iopub.status.idle":"2025-06-07T10:23:30.994700Z","shell.execute_reply.started":"2025-06-07T10:23:29.512801Z","shell.execute_reply":"2025-06-07T10:23:30.993936Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"theme_top_words = {}\n\nfor theme, sents in theme_sentences.items():\n    words = \" \".join(sents).lower().split()\n    common_words = Counter(words).most_common(10)\n    theme_top_words[theme] = [word for word, _ in common_words]\n\n# Display top words per theme\nprint(theme_top_words)","metadata":{"id":"ViUASrmvW8c2","outputId":"1f1a38df-03f0-40cf-a596-e388cee9507b","trusted":true,"execution":{"iopub.status.busy":"2025-06-07T10:25:55.139555Z","iopub.execute_input":"2025-06-07T10:25:55.139855Z","iopub.status.idle":"2025-06-07T10:25:55.165662Z","shell.execute_reply.started":"2025-06-07T10:25:55.139834Z","shell.execute_reply":"2025-06-07T10:25:55.164721Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load spaCy model for Named Entity Recognition\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Function to get BERT embeddings for words\ndef get_bert_embedding(word):\n    inputs = tokenizer(word, return_tensors=\"pt\").to(device) \n    with torch.no_grad():\n        outputs = model(**inputs)\n    return outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n\n# Prepare embeddings for theme keywords using BERT\ntheme_embeddings = {\n    theme: np.mean([get_bert_embedding(word) for word in keywords], axis=0)\n    for theme, keywords in themes.items()\n}\n","metadata":{"id":"Fy58pQAl0AKt","trusted":true,"execution":{"iopub.status.busy":"2025-06-07T10:30:12.547314Z","iopub.execute_input":"2025-06-07T10:30:12.547957Z","iopub.status.idle":"2025-06-07T10:30:13.312232Z","shell.execute_reply.started":"2025-06-07T10:30:12.547932Z","shell.execute_reply":"2025-06-07T10:30:13.311601Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Extract top words per theme using BERT similarity\ntheme_top_words = defaultdict(list)\nfor theme, sents in theme_sentences.items():\n    words = list(set(\" \".join(sents).lower().split()))  # Unique words\n    word_embeddings = {word: get_bert_embedding(word) for word in words}\n\n    # Compute cosine similarity to theme embedding\n    similarities = {\n        word: cosine_similarity([emb], [theme_embeddings[theme]])[0][0]\n        for word, emb in word_embeddings.items()\n    }\n\n    # Select top 10 words with highest similarity\n    sorted_words = sorted(similarities.items(), key=lambda x: x[1], reverse=True)[:50]\n    theme_top_words[theme] = [word for word, _ in sorted_words]\n\n# Display results\nfor theme, words in theme_top_words.items():\n    print(f\"\\nğŸ”¹ Theme: {theme.upper()}\")\n    print(\"Top Words:\", \", \".join(words))","metadata":{"id":"nnliPKAAleaJ","outputId":"42a88468-8cb7-4581-d905-658cdd3bb94e","trusted":true,"execution":{"iopub.status.busy":"2025-06-07T10:57:40.773231Z","iopub.execute_input":"2025-06-07T10:57:40.774025Z","iopub.status.idle":"2025-06-07T10:59:16.384312Z","shell.execute_reply.started":"2025-06-07T10:57:40.773983Z","shell.execute_reply":"2025-06-07T10:59:16.383636Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize Sentiment Analyzer\nsid = SentimentIntensityAnalyzer()\n\n# Sentiment summary\nfor theme, sents in theme_sentences.items():\n    if sents:\n        scores = [sid.polarity_scores(s)['compound'] for s in sents]\n        theme_sentiment[theme] = np.mean(scores)\n    else:\n        theme_sentiment[theme] = None\n\nfor theme, score in theme_sentiment.items():\n    print(f\"Theme: {theme}, Avg Sentiment: {score:.2f}\" if score is not None else f\"{theme}: No sentences found.\")\n","metadata":{"id":"0cFm5M370Bqk","outputId":"7c15f1fe-b3c0-4341-ec4d-2a8aef016de7","trusted":true,"execution":{"iopub.status.busy":"2025-06-07T11:00:54.413315Z","iopub.execute_input":"2025-06-07T11:00:54.413601Z","iopub.status.idle":"2025-06-07T11:00:55.234323Z","shell.execute_reply.started":"2025-06-07T11:00:54.413574Z","shell.execute_reply":"2025-06-07T11:00:55.233627Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for theme, sents in theme_sentences.items():\n    print(f\"\\nğŸ”· THEME: {theme.upper()}\")\n\n    if not sents:\n        print(\"No relevant sentences found.\")\n        continue\n\n    # Sentiment scores\n    sentiments = [sid.polarity_scores(s)['compound'] for s in sents]\n\n    # Create DataFrame for display\n    df = pd.DataFrame({'Sentence': sents, 'Sentiment Score': sentiments})\n\n    # Sort for top positive & negative sentences\n    df_sorted = df.sort_values(by='Sentiment Score', ascending=False)\n    top_positive = df_sorted.head(10)  # Top 10 positive sentences\n    top_negative = df_sorted.tail(10)  # Least 10 negative sentences\n\n    print(\"\\nâœ… **Top 10 Positive Sentences:**\")\n    display(top_positive)\n\n    print(\"\\nâŒ **Least 10 Negative Sentences:**\")\n    display(top_negative)\n\n    # **Bold Average Sentiment Score**\n    avg_sent = np.mean(sentiments)\n    print(f\"\\n\\033[1mğŸ”¸ Average Sentiment Score: {avg_sent:.2f}\\033[0m\\n\")\n\n    # **Use Precomputed Top Words for Word Cloud Instead of Frequency**\n    filtered_words = theme_top_words[theme]  # Reuse top BERT-extracted words\n\n    # Generate word cloud using **top 10 relevant words**\n    wordcloud = WordCloud(width=800, height=400, background_color='white', max_words=50)\n    wordcloud.generate(\" \".join(filtered_words))\n\n    plt.figure(figsize=(10, 5))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis(\"off\")\n    plt.title(f\"Word Cloud: {theme}\")\n    plt.show()","metadata":{"id":"T9wL0D8p3qEj","outputId":"a87b515c-c807-4865-cef9-cad5a7573a97","trusted":true,"execution":{"iopub.status.busy":"2025-06-07T11:02:19.881447Z","iopub.execute_input":"2025-06-07T11:02:19.882097Z","iopub.status.idle":"2025-06-07T11:02:22.121628Z","shell.execute_reply.started":"2025-06-07T11:02:19.882072Z","shell.execute_reply":"2025-06-07T11:02:22.120697Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"kmeans = KMeans(n_clusters=5, random_state=42)\nlabels = kmeans.fit_predict(X)\n\n# 1ï¸âƒ£ **Silhouette Score**\nsil_score = silhouette_score(X, labels)\nprint(f\"Silhouette Score: {sil_score:.2f}\")\n\n\n# 2ï¸âƒ£ **Elbow Method**\ninertia_values = []\ncluster_range = range(2, 10)\n\nfor k in cluster_range:\n    km = KMeans(n_clusters=k, random_state=42)\n    km.fit(X)\n    inertia_values.append(km.inertia_)\n\nplt.figure(figsize=(8, 5))\nplt.plot(cluster_range, inertia_values, marker='o', linestyle='--')\nplt.xlabel(\"Number of Clusters\")\nplt.ylabel(\"Inertia\")\nplt.title(\"Elbow Method for Optimal K\")\nplt.show()\n\n# 3ï¸âƒ£ **Cluster Distribution**\nsns.countplot(x=labels)\nplt.xlabel(\"Cluster Label\")\nplt.ylabel(\"Count\")\nplt.title(\"Cluster Distribution\")\nplt.show()\n\n# 4ï¸âƒ£ **2D Visualization using PCA**\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\n\nplt.figure(figsize=(8, 6))\nsns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=labels, palette=\"viridis\")\nplt.xlabel(\"Principal Component 1\")\nplt.ylabel(\"Principal Component 2\")\nplt.title(\"KMeans Clustering Visualization\")\nplt.legend(title=\"Cluster\")\nplt.show()","metadata":{"id":"o0kj5H8szvkH","outputId":"be024e7b-7716-42f9-f86b-508eba743f1b","trusted":true,"execution":{"iopub.status.busy":"2025-06-07T11:50:20.237402Z","iopub.execute_input":"2025-06-07T11:50:20.237678Z","iopub.status.idle":"2025-06-07T12:17:19.834365Z","shell.execute_reply.started":"2025-06-07T11:50:20.237657Z","shell.execute_reply":"2025-06-07T12:17:19.833677Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Review Starts Here**","metadata":{"id":"-qvCMbUjft-I"}},{"cell_type":"code","source":"from transformers import BertTokenizer, BertModel\nimport torch\nimport numpy as np\nfrom collections import defaultdict, Counter\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Load BERT model & tokenizer\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\nmodel = BertModel.from_pretrained(\"bert-base-uncased\")\n\n# Function to get BERT embeddings for words\ndef get_bert_embedding(word):\n    inputs = tokenizer(word, return_tensors=\"pt\")\n    with torch.no_grad():\n        outputs = model(**inputs)\n    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n\n# Prepare embeddings for theme keywords\ntheme_embeddings = {\n    theme: np.mean([get_bert_embedding(word) for word in keywords], axis=0)\n    for theme, keywords in themes.items()\n}\n\n# Extract top words per theme using BERT similarity\ntheme_top_words = defaultdict(list)\nfor theme, sents in theme_sentences.items():\n    words = list(set(\" \".join(sents).lower().split()))  # Unique words\n    word_embeddings = {word: get_bert_embedding(word) for word in words}\n\n    # Compute cosine similarity to theme embedding\n    similarities = {\n        word: cosine_similarity([emb], [theme_embeddings[theme]])[0][0]\n        for word, emb in word_embeddings.items()\n    }\n\n    # Select top 10 words with highest similarity\n    sorted_words = sorted(similarities.items(), key=lambda x: x[1], reverse=True)[:10]\n    theme_top_words[theme] = [word for word, _ in sorted_words]\n\n# Display results\nfor theme, words in theme_top_words.items():\n    print(f\"\\nğŸ”¹ Theme: {theme.upper()}\")\n    print(\"Top Words:\", \", \".join(words))\n","metadata":{"id":"61V9LLbJ3qv3","trusted":true,"execution":{"iopub.status.busy":"2025-05-25T07:33:26.708078Z","iopub.status.idle":"2025-05-25T07:33:26.708873Z","shell.execute_reply.started":"2025-05-25T07:33:26.708233Z","shell.execute_reply":"2025-05-25T07:33:26.708248Z"}},"outputs":[],"execution_count":null}]}