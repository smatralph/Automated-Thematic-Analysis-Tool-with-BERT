# -*- coding: utf-8 -*-
"""thematic-analysis-tool.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1p8YoUWHsSrAyL8motS17_7qypdHQTXfe
"""

# Commented out IPython magic to ensure Python compatibility.
# #@title 🔧 Install and Import Required Libraries
# %%capture
# # Install all required libraries
# #!pip install -q transformers torch sentencepiece ipywidgets
# #!pip uninstall -y gensim
# #!pip install --upgrade --force-reinstall gensim
# #!pip install numpy==1.26.4
# #!pip install gensim
# #!pip install -q pymupdf
# #!pip install nltk
# #!pip install --no-cache-dir spacy cython
# #!pip install kneed
# #!pip install numpy==1.26.4 --force-reinstall --no-cache-dir
# 
# # Import libraires
# from huggingface_hub import login
# import os, io, logging, requests
# import fitz  # PyMuPDF
# import nltk
# from sklearn.metrics import calinski_harabasz_score
# import contextlib
# from itertools import combinations
# import kagglehub
# from bs4 import BeautifulSoup
# from IPython.display import display, clear_output
# import ipywidgets as widgets
# from nltk.tokenize import sent_tokenize
# import re
# import string
# import spacy
# nltk.download('punkt')
# nltk.download('punkt_tab')
# nltk.download("vader_lexicon")
# from transformers import BertTokenizer, BertModel
# import torch
# import numpy as np
# from sklearn.cluster import KMeans
# from sklearn.metrics import silhouette_score
# import matplotlib.pyplot as plt
# from kneed import KneeLocator
# from collections import Counter
# from collections import defaultdict, Counter
# from sklearn.metrics.pairwise import cosine_similarity
# from nltk.sentiment import SentimentIntensityAnalyzer
# import pandas as pd
# from wordcloud import WordCloud
# from collections import Counter
# import matplotlib.pyplot as plt
# import seaborn as sns
# from sklearn.cluster import KMeans
# from sklearn.metrics import silhouette_score
# from sklearn.decomposition import PCA
# # from sentence_transformers import SentenceTransformer

#@title 🔧 Upload Data

# ─── Setup ─────────────────────────────────────────────────────────────────────
# Base directories & logging
base_dir = '/content/NLP_Thematic_Tool'
raw_dir  = os.path.join(base_dir, 'raw_data')
os.makedirs(raw_dir, exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s %(levelname)s %(message)s",
    handlers=[logging.FileHandler(os.path.join(base_dir, 'system.log')),
              logging.StreamHandler()]
)


# ─── Widgets ────────────────────────────────────────────────────────────────────
pdf_upload = widgets.FileUpload(
    accept='.pdf', multiple=True, description="Upload PDFs"
)
url_input = widgets.Textarea(
    placeholder='One URL per line', description='Web URLs:', layout=widgets.Layout(width='100%', height='100px')
)
process_btn = widgets.Button(description="Process Files", button_style='success')
output_box  = widgets.Output()

display(widgets.VBox([
    widgets.HTML("<h3>Step 1: Upload PDFs and/or enter URLs</h3>"),
    pdf_upload,
    url_input,
    process_btn,
    output_box
]))

# ─── Extraction Functions ──────────────────────────────────────────────────────
def extract_text_from_urls(lines):
    texts = []
    for idx, url in enumerate(lines, 1):
        try:
            resp = requests.get(url, timeout=10)
            soup = BeautifulSoup(resp.content, 'html.parser')
            txt  = soup.get_text(separator=' ', strip=True)
            texts.append(txt)
            logging.info(f"Scraped URL #{idx}: {url}")
        except Exception as e:
            logging.error(f"Error scraping {url}: {e}")
    return texts


def extract_text_from_pdfs(uploaded, batch_size=4):
    texts = []
    items = list(uploaded.items())

    for i in range(0, len(items), batch_size):
        batch = items[i:i + batch_size]
        print(f"📄 Processing batch {i // batch_size + 1} of {((len(items) - 1) // batch_size) + 1}...")

        for fname, info in batch:
            try:
                stream = io.BytesIO(info['content'])
                doc = fitz.open(stream=stream, filetype='pdf')
                text = " ".join(page.get_text() for page in doc)
                texts.append(text)
                logging.info(f"Extracted text from PDF: {fname}")
            except Exception as e:
                logging.error(f"Error processing {fname}: {e}")
                texts.append("")  # Append empty string to maintain alignment

    return texts


# ─── Button Callback ──────────────────────────────────────────────────────────
def on_process_clicked(btn):
    with output_box:
        clear_output()
        # 1. Gather PDF texts
        pdf_texts = extract_text_from_pdfs(pdf_upload.value)
        print(f"✔ PDFs processed: {len(pdf_texts)}")

        # 2. Gather URL texts (one per line)
        urls = [u.strip() for u in url_input.value.splitlines() if u.strip()]
        url_texts = extract_text_from_urls(urls)
        print(f"✔ URLs scraped:   {len(url_texts)}")

        # 3. Combine
        all_texts = pdf_texts + url_texts
        print(f"Total sources:    {len(all_texts)}\n")

        # Sanity check first 200 chars
        for i, doc in enumerate(all_texts, 1):
            print(f"--- Source #{i} preview ---")
            print(doc[:200].replace('\n',' ') + '...\n')

process_btn.on_click(on_process_clicked)

#@title 🔧 Add Stopwords

# Extract all text
pdf_texts = extract_text_from_pdfs(pdf_upload.value)
url_texts = extract_text_from_urls(url_input.value.strip().split('\n') if url_input.value else [])

# Define the file path for the TXT file

# Read the TXT file into a list
##with open(file_path, "r", encoding="utf-8") as f:
##    txt_texts = f.readlines()  # Reads each line as an item in a list

# Strip unnecessary spaces and filter empty lines
#txt_texts = [line.strip() for line in text_data.splitlines() if line.strip()]

# Merge all sources texts
all_texts = pdf_texts + url_texts #+ txt_texts

# Merge with excel texts
#all_texts = pdf_texts + url_texts + excel_texts

def preprocess(text):
    # Replace paragraph breaks with a space so sentences remain intact
    text = text.replace("\n", " ")

    # Preserve punctuation while removing unwanted symbols
    text = re.sub(r'[^a-zA-Z0-9.,!?;:\s]', '', text)

    return text.lower()


sentences = []
for doc in all_texts:
    preprocessed = preprocess(doc)
    sentences.extend(sent_tokenize(preprocessed))  # Proper sentence splitting

# Function to filter irrelevant sentences using spaCy

# Load spaCy model for Named Entity Recognition
nlp = spacy.load("en_core_web_sm")


def is_relevant(sentence):
    doc = nlp(sentence)

    # Count named entities (proper nouns, organizations, etc.)
    entity_count = sum(1 for ent in doc.ents if ent.label_ in ["ORG", "PERSON", "GPE", "PRODUCT"])

    # Apply POS tagging to remove sentences dominated by function words
    pos_tags = [token.pos_ for token in doc]
    relevant_pos = sum(1 for tag in pos_tags if tag in ["NOUN", "VERB", "ADJ"])  # Keep nouns, verbs, adjectives

    # Define relevance criteria
    return entity_count < 2 and relevant_pos >= 3  # Adjust thresholds as needed

# Apply spaCy filtering to remove irrelevant sentences
filtered_sentences = [s for s in sentences if is_relevant(s)]
print(f"Total sentences extracted: {len(filtered_sentences)}")


#-----------ADD STOPWORDS------------

from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS

stopword_widget = widgets.Textarea(
    value='',
    placeholder='Enter extra stopwords separated by commas',
    description='Stopwords:',
    layout=widgets.Layout(width='100%', height='60px')
)
display(stopword_widget)

def get_custom_stopwords():
    additional = stopword_widget.value.strip()
    custom_words = set(map(str.strip, additional.split(','))) if additional else set()
    return set(ENGLISH_STOP_WORDS).union(custom_words)

#@title 🔧 Add themes and keywords
themes_widget = widgets.Textarea(
    value='',  # Make it empty by default
    placeholder='theme_name: keyword1, keyword2, ...',
    description='Themes:',
    layout=widgets.Layout(width='100%', height='150px')
)
display(themes_widget)

def parse_themes():
    raw = themes_widget.value.strip().split('\n')
    theme_dict = {}
    for line in raw:
        if ':' in line:
            theme, keywords = line.split(':', 1)
            theme_dict[theme.strip()] = [k.strip().lower() for k in keywords.split(',')]
    return theme_dict

#@title 🤖 Process Text (Thematic Analysis)


# ─── Suppress BERT Setup and Embedding Computation ───
f = io.StringIO()
with contextlib.redirect_stdout(f), contextlib.redirect_stderr(f):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
    model = BertModel.from_pretrained("bert-base-uncased").to(device)

    def compute_embeddings(sentences, batch_size=35):
        embeddings = []
        for i in range(0, len(sentences), batch_size):
            batch = sentences[i:i + batch_size]
            inputs = tokenizer(batch, padding=True, truncation=True, return_tensors="pt").to(device)
            with torch.no_grad():
                outputs = model(**inputs)
            embeddings.extend(outputs.last_hidden_state[:, 0, :].cpu().numpy())
        return np.array(embeddings)

    sentence_embeddings = compute_embeddings(filtered_sentences, batch_size=35)


# ─── Optional: Dimensionality Reduction ───
# Only apply PCA if needed (e.g., for clustering stability or visualization)
apply_pca = True
if apply_pca:
    pca = PCA(n_components=20)
    X = pca.fit_transform(sentence_embeddings)
else:
    X = sentence_embeddings

# ─── Find Optimal K ───
def find_optimal_k(X, max_k=10):
    inertia_values = []
    cluster_range = range(2, max_k + 1)

    for k in cluster_range:
        km = KMeans(n_clusters=k, random_state=42)
        km.fit(X)
        inertia_values.append(km.inertia_)

    kn = KneeLocator(cluster_range, inertia_values, curve="convex", direction="decreasing")
    optimal_k = kn.knee or 5  # fallback to 5 if knee not found

    #plt.figure(figsize=(8, 5))
    #plt.plot(cluster_range, inertia_values, marker='o', linestyle='--')
    #plt.axvline(optimal_k, color='r', linestyle="--", label=f"Optimal K = {optimal_k}")
    #plt.xlabel("Number of Clusters (K)")
    #plt.ylabel("Inertia")
    #plt.title("Elbow Method for Optimal K")
    #plt.legend()
    #plt.show()

    return optimal_k

# ─── Clustering ───
optimal_k = find_optimal_k(X)

kmeans = KMeans(n_clusters=optimal_k, random_state=42)
labels = kmeans.fit_predict(X)

# ─── Evaluation ───
sil_score = silhouette_score(X, labels)
print(f"🔹 Silhouette Score: {sil_score:.2f}")

# ─── Cluster Assignment ───
clustered_sentences = {i: [] for i in range(kmeans.n_clusters)}
for label, sentence in zip(labels, filtered_sentences):
    clustered_sentences[label].append(sentence)

# ─── Display Clusters ───
#for cluster, sentences in clustered_sentences.items():
    #print(f"\n🔹 Cluster {cluster}: {len(sentences)} sentences")
    #print("\n".join(sentences[:5]))  # Show first 5 sentences per cluster







theme_sentences = defaultdict(list)

for cluster, sentences in clustered_sentences.items():
    theme_sentences[f"Theme_{cluster}"] = sentences

# Display clusters as themes
#for theme, sentences in theme_sentences.items():
    #print(f"\n🔹 {theme}: {len(sentences)} sentences")

stopwords = get_custom_stopwords()
themes = parse_themes()
theme_sentences = defaultdict(list)
theme_sentiment = {}

theme_top_words = {}

for theme, sents in theme_sentences.items():
    words = " ".join(sents).lower().split()
    common_words = Counter(words).most_common(10)
    theme_top_words[theme] = [word for word, _ in common_words]

# Display top words per theme
# print(theme_top_words)

# Load spaCy model for Named Entity Recognition
nlp = spacy.load("en_core_web_sm")

# Function to get BERT embeddings for words
def get_bert_embedding(word):
    inputs = tokenizer(word, return_tensors="pt").to(device)
    with torch.no_grad():
        outputs = model(**inputs)
    return outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()



# Prepare embeddings for theme keywords using BERT
theme_embeddings = {
    theme: np.mean([get_bert_embedding(word) for word in keywords], axis=0)
    for theme, keywords in themes.items()
}

# Assign sentences to themes based on embedding similarity
for sentence, emb in zip(filtered_sentences, sentence_embeddings):
    similarities = {
        theme: cosine_similarity([emb], [theme_emb])[0][0]
        for theme, theme_emb in theme_embeddings.items()
    }
    best_theme = max(similarities, key=similarities.get)
    if similarities[best_theme] > 0.5:  # You can tune this threshold
        theme_sentences[best_theme].append(sentence)




# Extract top words per theme using BERT similarity
theme_top_words = defaultdict(list)
for theme, sents in theme_sentences.items():
    words = list(set(
        w.strip(string.punctuation)
        for w in " ".join(sents).lower().split()
        if w.strip(string.punctuation) not in stopwords and w.strip(string.punctuation)
    ))

    word_embeddings = {word: get_bert_embedding(word) for word in words}

    # Compute cosine similarity to theme embedding
    similarities = {
        word: cosine_similarity([emb], [theme_embeddings[theme]])[0][0]
        for word, emb in word_embeddings.items()
    }

    # Select top 10 words with highest similarity
    sorted_words = sorted(similarities.items(), key=lambda x: x[1], reverse=True)[:50]
    theme_top_words[theme] = [word for word, _ in sorted_words]

# Initialize Sentiment Analyzer
sid = SentimentIntensityAnalyzer()

# Sentiment summary
for theme, sents in theme_sentences.items():
    if sents:
        scores = [sid.polarity_scores(s)['compound'] for s in sents]
        theme_sentiment[theme] = np.mean(scores)
    else:
        theme_sentiment[theme] = None


# Display results
for theme, words in theme_top_words.items():
    print(f"\n🔹 Theme: {theme.upper()}")
    print("Top Words:", ", ".join(words))


for theme, sents in theme_sentences.items():
    print(f"\n🔷 THEME: {theme.upper()}")

    if not sents:
        print("No relevant sentences found.")
        continue

    # ✅ Move this inside the loop so each theme gets its own words
    filtered_words = theme_top_words[theme]

    wordcloud = WordCloud(
        width=800,
        height=400,
        background_color='white',
        max_words=50
    )
    wordcloud.generate(" ".join(filtered_words))
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis("off")
    plt.title(f"Word Cloud: {theme}")
    plt.show()

#@title 🤖 Process Text (Sentiment Analysis)

# Ensure full sentence visibility
pd.set_option('display.max_colwidth', None)

for theme, sents in theme_sentences.items():
    print(f"\n🔷 THEME: {theme.upper()}")

    if not sents:
        print("No relevant sentences found.")
        continue

    # Sentiment scores
    sentiments = [sid.polarity_scores(s)['compound'] for s in sents]

    # Create DataFrame
    df = pd.DataFrame({'Sentence': sents, 'Sentiment Score': sentiments})

    # Sort for top positive & negative
    df_sorted = df.sort_values(by='Sentiment Score', ascending=False)
    top_positive = df_sorted.head(10)
    top_negative = df_sorted.tail(10)

    print("\n✅ **Top 10 Positive Sentences:**")
    display(top_positive)

    print("\n❌ **Least 10 Negative Sentences:**")
    display(top_negative)

    avg_sent = np.mean(sentiments)
    print(f"\n\033[1m🔸 Average Sentiment Score: {avg_sent:.2f}\033[0m\n")

#@title Validation and Reliability Check

print(f"\033[1m🔹 Optimal K selected: {optimal_k}\033[0m")

kmeans = KMeans(n_clusters=6, random_state=42)
labels = kmeans.fit_predict(X)


# 1️⃣ **Silhouette Score**
sil_score = silhouette_score(X, labels)
print(f"\033[1m🔹 Silhouette Score: {sil_score:.2f}\033[0m")

# Calinski-Harabasz Index
ch_score = calinski_harabasz_score(X, labels)
print(f"\033[1m📊 Calinski-Harabasz Score: {ch_score:.2f}\033[0m")


# 2️⃣ **Elbow Method**
inertia_values = []
cluster_range = range(2, 10)

for k in cluster_range:
    km = KMeans(n_clusters=k, random_state=42)
    km.fit(X)
    inertia_values.append(km.inertia_)

# Plot the elbow curve with the optimal K marked
plt.figure(figsize=(8, 5))
plt.plot(cluster_range, inertia_values, marker='o', linestyle='--', label='Inertia')
plt.axvline(optimal_k, color='red', linestyle='--', label=f'Optimal K = {optimal_k}')
plt.xlabel("Number of Clusters")
plt.ylabel("Inertia")
plt.title("Elbow Method for Optimal K")
plt.legend()
plt.grid(True)
plt.show()

# 3️⃣ **Cluster Distribution**
sns.countplot(x=labels)
plt.xlabel("Cluster Label")
plt.ylabel("Count")
plt.title("Cluster Distribution")
plt.show()

# 4️⃣ **2D Visualization using PCA**
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

plt.figure(figsize=(8, 6))
sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=labels, palette="viridis")
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.title("KMeans Clustering Visualization")
plt.legend(title="Cluster")
plt.show()




#Theme Coherence

def average_intra_theme_similarity(sentences, embeddings):
    if len(sentences) < 2:
        return None
    indices = [i for i, s in enumerate(filtered_sentences) if s in sentences]
    vectors = [sentence_embeddings[i] for i in indices]
    pairs = list(combinations(vectors, 2))
    sims = [cosine_similarity([a], [b])[0][0] for a, b in pairs]
    return np.mean(sims)

for theme, sents in theme_sentences.items():
    score = average_intra_theme_similarity(sents, sentence_embeddings)
    if score is not None:
        print(f"\033[1m🧠 Theme Coherence ({theme}): {score:.2f}\033[0m")